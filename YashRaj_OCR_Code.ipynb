{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning TrOCR with TensorFlow (Handwriting Recognition)\n",
    "\n",
    "This notebook walks through the complete pipeline for fine-tuning a Transformer-based OCR model (`TrOCR`) using TensorFlow on IAM/Imgur5K handwriting datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install transformers datasets tensorflow opencv-python jiwer -q\n",
    "!pip install -U sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check TensorFlow & GPU\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "IMG_SIZE = (384, 384)\n",
    "def preprocess_image_tf(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = tf.image.rgb_to_grayscale(img)\n",
    "    img = img / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text labels using TrOCR processor\n",
    "from transformers import TrOCRProcessor\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "def tokenize_label(text):\n",
    "    return processor.tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"tf\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Load IAM and Imgur5K Datasets\n",
    "# -------------------------------------\n",
    "\n",
    "# IAM Handwriting Dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# This loads line-level annotations for IAM handwriting\n",
    "iam_dataset = load_dataset(\"iam_dataset\", split=\"train[:90%]\")\n",
    "iam_val_dataset = load_dataset(\"iam_dataset\", split=\"train[90%:]\")\n",
    "\n",
    "# Download Imgur5K manually and unzip to a folder in Drive or local path\n",
    "# Assuming /content/Imgur5K/ folder with 'images' and 'labels.csv'\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "IMGUR_PATH = \"/content/Imgur5K\"\n",
    "labels_df = pd.read_csv(os.path.join(IMGUR_PATH, \"labels.csv\"))  # columns: ['image_path', 'text']\n",
    "\n",
    "imgur_image_paths = [os.path.join(IMGUR_PATH, p) for p in labels_df[\"image_path\"]]\n",
    "imgur_labels = labels_df[\"text\"].tolist()\n",
    "\n",
    "# Convert IAM to filepaths and labels (line-level)\n",
    "iam_image_paths = [example['image']['path'] for example in iam_dataset]\n",
    "iam_labels = [example['text'] for example in iam_dataset]\n",
    "\n",
    "val_image_paths = [example['image']['path'] for example in iam_val_dataset]\n",
    "val_labels = [example['text'] for example in iam_val_dataset]\n",
    "\n",
    "# Combine datasets for training\n",
    "train_image_paths = iam_image_paths + imgur_image_paths\n",
    "train_texts = iam_labels + imgur_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TrOCR model (TensorFlow version)\n",
    "from transformers import TFVisionEncoderDecoderModel\n",
    "model = TFVisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\", from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "def compute_loss(y_true, y_pred):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    return loss_fn(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), loss=compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model (fill in your train_ds, val_ds first)\n",
    "# train_ds = load_dataset(train_image_paths, train_texts).batch(4)\n",
    "# val_ds = load_dataset(val_image_paths, val_texts).batch(4)\n",
    "# model.fit(train_ds, validation_data=val_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation (CER and WER)\n",
    "from jiwer import cer, wer\n",
    "def evaluate_model(model, processor, test_images, test_labels):\n",
    "    predictions, ground_truth = [], []\n",
    "    for img_path, true_text in zip(test_images, test_labels):\n",
    "        img = preprocess_image_tf(img_path)\n",
    "        img = tf.expand_dims(img, axis=0)\n",
    "        pixel_values = processor(images=img.numpy(), return_tensors=\"tf\").pixel_values\n",
    "        generated = model.generate(pixel_values)\n",
    "        pred_text = processor.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "        predictions.append(pred_text)\n",
    "        ground_truth.append(true_text)\n",
    "    return cer(ground_truth, predictions), wer(ground_truth, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_pretrained(\"/content/trocr-tf-finetuned\")\n",
    "processor.save_pretrained(\"/content/trocr-tf-finetuned\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Fine_Tune_TrOCR_TensorFlow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
